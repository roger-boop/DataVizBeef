---
title: "home-assignment"
author: "Oscar Casals, Roger Bosch, Albert Vidal"
date: "2022-11-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

For this project we will use three data frames containing the expression different genes have in certain cells after being exposed to some drugs in order to determine if there is a big disparity between the effects of each substance or instead they have near identical consequences.

To aid us in this task we have a file containing the metadata of each sample, in specific their cell type and the drug it has been subjected to.

The tools we will use to visualise our data are: PCA(principal component analysis) and tSNE(t-Distributed Stochastic Neighbor Embedding).
 
With all the materials defined let's get going with the analysis.

## Q1

**Load the data from the three plates. Which command do you use? Provide code alternatives and prove that the data is loaded.**

Our first step in this analysis is to load the data contained in the tsv files, an easy way to perform this task is with a simple `read.table()`.

```{r q1-loads, error=TRUE}
P3 <- read.table('P3.tsv', header = TRUE)
P4 <- read.table('P4.tsv', header = TRUE)
P5 <- read.table('P5.tsv', header = TRUE)
```

This method but has the inconvenience of requiring to decompress the data, which might not be a good idea when working with large files, that is why we will use the `gzfile()` function to avoid this problem.

```{r q1-loads-alt, error=TRUE}
P3_2 <- read.table(gzfile("P3.tsv.gz"), header = TRUE)
P4_2 <- read.table(gzfile("P4.tsv.gz"), header = TRUE)
P5_2 <- read.table(gzfile("P5.tsv.gz"), header = TRUE)
```

While read.table is the standard to import any table into r, there are some other functions we can use to import tsv files:

  - We could use `read_tsv()`, a function specialized in loading tsv files into R.

```{r q1-loads-alt2, error=TRUE}
library(readr)
P3_3 <- read_tsv(gzfile("P3.tsv.gz"), col_names = TRUE)
P4_3 <- read_tsv(gzfile("P4.tsv.gz"), col_names = TRUE)
P5_3 <- read_tsv(gzfile("P5.tsv.gz"), col_names = TRUE)
```

  - We could also treat our data like a csv file and load it into R using `read_csv()`. In this case we must specify that each column is separated by a tab in order to prevent the function of returning a table with just one column.

```{r, error=TRUE}
P3_4 <- read.csv(gzfile("P3.tsv.gz"), header = TRUE, sep = "\t")
P4_4 <- read.csv(gzfile("P4.tsv.gz"), header = TRUE, sep = "\t")
P5_4 <- read.csv(gzfile("P5.tsv.gz"), header = TRUE, sep = "\t")
```

  - Finally we can take advantage of the fact that tsv files are delimited text files to load our data using `read.delim()`.

```{r, error=TRUE}
P3_5 <- read.delim(gzfile("P3.tsv.gz"), header = TRUE)
P4_5 <- read.delim(gzfile("P4.tsv.gz"), header = TRUE)
P5_5 <- read.delim(gzfile("P5.tsv.gz"), header = TRUE)
```

Now we will check that the data has been loaded as expected using `head()`, a function that prints the first few lines of the data frame it receives as input. Since we are testing all the methods used to load the data showing the output of the following chunk would fill the html with non-relevant information, that is why you will have to trust us when we say the code worked as expected or replicate what we have done until now to test it.

```{r q1-evidence, error=TRUE, results='hide'}
#Data loaded with read.table
head(P3)
head(P4)
head(P5)
#Data loaded with read_tsv
head(P3_3)
head(P4_3)
head(P5_3)
#Data loaded with read.csv
head(P3_4)
head(P4_4)
head(P5_4)
#Data loaded with read.delim
head(P3_5)
head(P4_5)
head(P5_5)
```

Right now our tables have the name of the genes as row names and the sample names as colnames, this makes our data unintuitive to navegate so we will set the gene column as rownames, remove it since it is no longer necessary, and transpose the tables so everything looks as we want to. 


```{r q1-transpose_matrix}
rownames(P3) <- P3$gene
P3$gene <- NULL
rownames(P4) <- P4$gene
P4$gene <- NULL
rownames(P5) <- P5$gene
P5$gene <- NULL

P3 <- t(P3)
P4 <- t(P4)
P5 <- t(P5)
```

## Q2

**Load the treatment associated with every cell. Which command do you use? How many different conditions are present in the plates? How many cells in each condition? Do you need to reorder cells so that data in sample sheet and tsv files match?**

The treatment associated to each cell can be found in the samplesheet.tsv, to load it we can use the `read.table()` command again.

```{r q2-packages, message=FALSE}
library(dplyr)
library(tidyr)
```

```{r q2-load}
treatments <- read.table('samplesheet.tsv', header = TRUE)
```

The table loaded contains cell types and conditions inside the column label, this is too much information for just a column so we will separate both features using the functions %>% and separate from the packages dplyr and tidyr respectively.

```{r q2-separate}
treatments <- treatments %>% separate(col = label, into = c("celltypes", "conditions"), sep = "\\+")
```

With the conditions separated from celltypes we can easily check how many we have using the table function, which creates a table containing the number of times a condition appears in treatments.

```{r q2-cond}
table(treatments$conditions)
```

**Thanks to the table above we know there are 3 different conditions present in the plates.**

**The same table can be used to know how many cells are present in each condition.**

```{r q2-cellconds}
table(treatments$conditions, treatments$celltypes)
```

Finally we need to figure out if we need to record cell names or not by checking if the indexes for each cell are the same in all the dataframes created. If they do, then we can remove them since we will already know which cell each column is referring to by binding the metadata with the tsv files, otherwise we will have to record it.

Our way of knowing wether the indexes align in all dataframes or not is by looking for false values in a condition that compares the rownames of P3, P4 and P5 with the cellnames present in treatment, if there are no false values then the posterior table should just have a column called true with a value of 288(the number of rows in both treatments and in P3, P4 and P5 combined).


```{r q3-record_cell_or_not, echo=T}
table(treatments$cellnames == cbind(rownames(P3), rownames(P4), rownames(P5)))
```

**As we can see no False boolean appears which means all the indexes are identical and therefore, there is no need to record the cell names.**

Taking advantatge of the fact we are working with the treatments dataframe we are going to add a new column that contains the plate from which each cell comes from. Eventhough this new feature might seem useless right now it will be of use later to check if there is batch effect or not.

To create this new column we used rep to generate the vectors that will conform the new feature and append to put it all together and add the new column to the dataframe.

```{r bind}
treatments$replicates <- append(append(rep("P3", 96), rep("P4",96)), rep("P5", 96))
```

To end this exercise we will create a data frame with all the information obtained till now, this will allow us to use our data visualization tools on all the information at our disposal.

```{r final_data}
X <- rbind(P3, P4, P5)
```

## Q3

**Represent the data with a PCA projection and tSNE. Scale and normalize the data appropriately, showing the different steps in the process and compare with the raw data. Using colors, argue whether batch effects are present between the three plates. Provide different visualizations and/or code alternatives.**

With all the data in a whole dataframe we can start plotting it. We will first perform a PCA.

```{r pca-non-norm}
library(ggplot2)
library(Rtsne)
pca <- stats::prcomp(X)
```

Then we will create a function in charge of reformatting the output of prcomp so it is easier to plot.

```{r}
make_components <- function(pca)
{
  return (as_tibble(pca$x) %>% bind_cols(treatments) %>% as.data.frame())
}
```

Now everything is ready to plot the data, each dot in our graph will be colored depending on the plate it comes from for the purpose of checking whether there is batch effect or not.

```{r}
components <- make_components(pca)
ggplot(components, aes(x = PC1, y = PC2, color = replicates)) + geom_point() + theme_light() + labs(color = "Replicate", title = "PCA non-normalized")
```

**There is a clear batch effect, mostly among the P3 replicate, as they are mostly clustered away from the P4 and P5 clusters. Normalizing and scaling the data might fix this issue.**

To normalize the data we will first remove all the zeroes in it and then average it by the total expression of each cell. Since this operation is going to be repeated a lot during this assignment we will create a function that performs it.

```{r scale-func}
my.normalize <- function(X)
{
  X_nozero <- X[, colSums(X)>0]
  return(X_nozero/rowSums(X_nozero))
}
```

With the function created, let's normalize our dataset.

```{r scaling}
Y = my.normalize(X)
```

Now it is time to plot our normalized dataset, we will set scale = TRUE in prcomp so our data is scaled before it is plotted. As we have done before we will color each dot in function of the plate they come from in order to confirm the lack of batch effect.

```{r pca-normalised}
pca_normalized <- stats::prcomp(Y, scale = TRUE)

components <- make_components(pca_normalized)

ggplot(components, aes(x = PC1, y = PC2, color = replicates)) + geom_point() + theme_light() + labs(color = "Replicates", title = "PCA normalized")
```

**By normalising and scaling the data we have removed the batch effect, but there is still one huge otulier along PC1 in both graphs that needs to be removed.**

To remove the outlier we first need to figure out it's name, we will find it by using the fviz_pca_ind from the factoextra package to create a graph similar to the previous one but with the name of each dot.

```{r outlier-plot}
#library(factoextra)

#fviz_pca_ind(pca_normalized, repel = T)
```

It is clear our outlier is the cell P2771_N704.S506. To remove it we will remove the row named after this cell by using an %in% statement and the filter function from dplyr package, then we will normalize X again.

```{r remove-outlier, results='hide'}
X <- X[! rownames(X) %in% 'P2771_N704.S506',]
treatments <- treatments %>% filter(!cellnames %in% 'P2771_N704.S506')
Y <- my.normalize(X)
```

With the outlier out of the way our graph should look much better now to the point each cluster is differentiable form the others.

```{r pca-normalized-no-outlier}
pca_normalized <- stats::prcomp(Y, scale = TRUE)
components <- make_components(pca_normalized)
ggplot(components, aes(x = PC1, y = PC2, color = replicates)) + geom_point() + theme_light() + labs(color = "Replicates", title = "PCA normalized")
```


We have already seen normalisation and scaling is needed to have a good PCA, but does the same thing apply to tSNE? Let's figure it out:

  - We will use the Rtsne function to perform a tSNE on both the raw data and the normalised one. 

```{r tsnes}
X_tsne <- Rtsne(X)
Y_tsne <- Rtsne(Y)
```

  - Similar to prcomp the output of Rtsne needs to be modified so it can be easily plotted with ggplot2, we will build a function that creates a data frame with set modifications. 

```{r tsne-func}
make_tsne_plot <- function(tsne)
{
  tsne_plot <- data.frame(x = tsne$Y[,1], 
                        y = tsne$Y[,2], 
                        col = treatments$replicates,
                        sha = treatments$conditions,
                        alp = treatments$celltypes)
  return(tsne_plot)
}
```

  - tSNE is a heuristic algorithm, which means that it will return a different plot every time we execute it. So, let's fix a seed so we can have reproducible results.

```{r seed}
set.seed(123)
```

  - With the tSNE created we will make a plot that, as all the others created until now, will have each cell colored in function of the plate they come from so as to check for batch effect.

```{r tsne-non-normalized}
tsne_plot <- make_tsne_plot(X_tsne)
ggplot(tsne_plot) + geom_point(aes(x=x, y=y, color=col)) + theme_light() + labs(color = "Replicates", title = "tSNE non-normalized")
```

**As it happened with PCA the graph above has batch effect, therefore we will need to normalize the data in order to remove it.**

```{r tsne-normalized}
tsne_plot <- make_tsne_plot(Y_tsne)
ggplot(tsne_plot) + geom_point(aes(x=x, y=y, color=col)) + theme_light() + labs(color = "Replicates", title = "tSNE normalized") 
```

While we have gotten rid of tha batch effect in both PCA and tSNE the graphs obtained do not look alike, PCA has most of it's clusters arround the midle of the plot while tSNE has the clusters more distributed arround the graph, this could be attributed to the fact that the perplexity parameter in tSNE is too small which reduces the number of neighbours a point can have.

## Q4

**Check the effect of perplexity and iterations, the same as scaling the data for tSNE and PCA plots respectively.**

tSNE has perplexity and iteration parameters that we have left as default until now, let's check how they affect the results starting from perplexity.

To observe the effects of perplexity we will create a for loop that changes perplexity by 3, 60 and 70 creating a plot for each iteration.

```{r perplex}
for (i in c(3, 60, 70))
{
  i_tsne <- Rtsne(Y, perplexity = i)
  tsne_plot <- make_tsne_plot(i_tsne)
  print(ggplot(tsne_plot) + geom_point(aes(x=x, y=y, color=col)) + theme_light() + labs(color = "Replicates", title = paste0("tSNE for perplexity = ", i)))
}
```

**The perplexity changes the number of neighbors that each cluster should have. For perplexity = 3, the clusters only have 3 neighbors, so the clusters are tiny. It's good to have a high perplexity since this will turn our clusters into denser structures.**

Now is time to check the effect of the iterations, we will follow the same procedure as before changing the max_iter parameter by 5, 500 and 2000, plotting the results in each iteration.

```{r iter}
for (i in c(5, 500, 2000))
{
  i_tsne <- Rtsne(Y, max_iter = i)
  tsne_plot <- make_tsne_plot(i_tsne)
  print(ggplot(tsne_plot) + geom_point(aes(x=x, y=y, color=col)) + theme_light() + labs(color = "Replicates", title = paste0("tSNE for iterations = ", i)))
}
```

**A lower number of iterations leads to one singular big cluster. The more iterations, the more distinct the clusters are. A higher number is better in this case since being able to differentiate clusters is what will allow us to reach a conclusion.**

## Q5

**Argue whether the drugs SAHA and PMA have the same effect on the cells. Add a legend and label the axes accordingly.**

Now that we have the data looking as we want to and we have observed how tSNE and PCA are affected by different parameters it is time to start extracting conclusions from our data, in particular we'll start answering our initial question in the introduction with SAHA and PMA, do they have the same effect?

To answer it we will create a subset from our data that will only contain the cells with a SAHA or PMA condition by using a == condition on the treatments$conditions column. Then we will normalise the subset and perform a PCA in order to see if the two drugs share a cluster, if they do then it means both substances have the same effect.

```{r}
X_SAHA_PMA <- X[((treatments$conditions == "SAHA") == TRUE) | ((treatments$conditions == "PMA")== TRUE),]

treat_SAHA_PMA <- treatments[(treatments$conditions == "SAHA") | (treatments$conditions == "PMA"),]

Y_SAHA_PMA <- my.normalize(X_SAHA_PMA)

PCA_filtered <- stats::prcomp(Y_SAHA_PMA, scale = TRUE)

components_filtered <- components[(components$conditions == "SAHA") | (components$conditions == "PMA"),]

ggplot(components_filtered, aes(x = PC1, y = PC2, color = conditions, shape = celltypes)) + geom_point() + theme_light() + scale_shape_manual(values = c(3,17)) +  labs(color = "Conditions", shape = "Cell type")

```

**Both SAHA and PMA have clearly differentiable clusters therefore they do not have the same effect on cells**

## Q6

**Using the correct PCA analysis, find an interpretation for the first and the second principal component.**

If we are to extract conclusions from the PCA analyses we perform it would we wise to figure out what PC1 and PC2 use to divide our data into clusters.

To figure this out we will create a function that will return the total expression of the dataset it receives as input and use the components pca object created earlier to make a plot that will show the total expression and the drugs used on each cell.

```{r totalexpr-func}
totalExpr <- function(X)
{
  X_nozero <- X[, colSums(X)>0]
  return (rowSums(X_nozero))
}
```

```{r}
pl <- ggplot(components, aes(x = PC1, y = PC2, color = totalExpr(X), shape = conditions)) + geom_point() + scale_colour_gradient(low = "#00FF00", high = "#FF0000") + scale_shape_manual(values = c(3,17, 11)) + theme_light() + labs(color = "Expression", shape = "Conditions", title = "PCA")

pl
```

**It is evident on the plot above that PC1 separates cells based on the conditions they have been exposed to while PC2 differentiates them by their total expression.** 

## Q7

**Using your answer to the previous question, find three genes that are activated by SAHA but not by PMA.**

Now is finally time to apply what we have learned from observing the two last PCA plots to extract three genes activated by SAHA but not by PMA.

In order to do this, we will subset all cells with SAHA and PMA respectively and sum all the expressions of their genes. This is not the total expression of the cell, but the total expression of each gene. Then, we take the 3 most expressed genes in SAHA and the 3 most expressed genes in PMA, and compare them.

```{r}
SAHA_cells <- treatments %>% filter(conditions == "SAHA") %>% pull(cellnames)
PMA_cells <- treatments %>% filter(conditions == "PMA") %>% pull(cellnames)
SAHA_Y <- Y[SAHA_cells,]
PMA_Y <- Y[PMA_cells,]

names(colSums(SAHA_Y)[order(colSums(SAHA_Y), decreasing=TRUE)[1:3]])
names(colSums(PMA_Y)[order(colSums(PMA_Y), decreasing=TRUE)[1:3]])
```

After this procedure we see that the 3 most expressed genes in SAHA are not the same as in PMA. Therefore, we can claim that 


ENSG00000251562.8  ENSG00000198804.2  ENSG00000280614.1


Are activated by SAHA but not by PMA.

# Conclusions extracted until now

  Up until now we have seen how PCA and tSNE are both useful data visualisation algorithms with their own advantatges and disadvantatges. For this particular case we have leaned towards PCA due to it's high speed when compared to tSNE, which is an important factor to take into account when we have performed 3 PCAs to extract some information about out data.
  
  Regarding our initial question it seems that PMA has the same effect as DMSO on cells which differs from the consequences brought by SAHA.
  
  Having said all this we still have not explored how UMAP interacts with our data, which not only might provide similar or different plots but also allow us to make predictions. For the following Q8, Q9 we will see how UMAP interacts with our data and in Q10 we will compare it to PCA and tSNE.


## Q8

**Create a UMAP visualization of the results. Explore differences between producing**
**results from raw data counts or normalized results. Explore also the effect of predicting**
**a new sample result from previous UMAP analysis. Compare results and give some**
**thoughts on it.**

To apply the umap algorithm we will use the umap function,

```{r libraries-2}
library(umap)
```

As with tSNE and PCA, umap's oputput requires some filtering to be ploted, to make this procedure easier, we have made a function that automatically takes a UMAP analysis and re-formats it so it can work well with ggplot.

```{r umap-func}
make_umap_data <- function(UMAP)
{
  return( data.frame(x = UMAP$layout[,1], y = UMAP$layout[,2], col = treatments) )
}
```

Let's do both UMAPs, for normalized and non-normalized data, and compare them.

```{r umap-1}
UMAP_X <- umap::umap(X)
UMAP_Y <- umap::umap(Y)
UMAP_data_X <- make_umap_data(UMAP_X)
UMAP_data_Y <- make_umap_data(UMAP_Y)
ggplot(UMAP_data_X, aes(x = x, y = y, shape = col.conditions, color=col.replicates)) + geom_point() + scale_shape_manual(values = c(3,17, 11)) + theme_light() + labs(color = "Expression", shape = "Conditions", title = "Non-scaled UMAP")
ggplot(UMAP_data_Y, aes(x = x, y = y, shape = col.conditions, color=col.replicates)) + geom_point() + scale_shape_manual(values = c(3,17, 11)) + theme_light() + labs(color = "Expression", shape = "Conditions", title = "Normalized UMAP")
```

DISCUSS DIFFERENCES OF NORMALIZED/NON NORMALIZED

Now that we have done that, we can recreate the other plots we performed but with UMAP and see if the results match.

```{r umap-2}
ggplot(UMAP_data_Y, aes(x = x, y = y, shape = col.conditions, color=totalExpr(X))) + geom_point() + 
  scale_colour_gradient(low = "#00FF00", high = "#FF0000") + scale_shape_manual(values = c(3,17, 11)) + theme_light() + 
  labs(color = "Expression", shape = "Conditions", title = "Normalized UMAP")
```

While the clusters are distributed differently, the clusters themselves are the same. There are SAHA, PMA and DMSO clusters all the same, like in the other plots we had done.

Next, let's make some predictions with UMAP. We will re-do the plot above but with all predicted data, using the UMAP model we used on the dataset Y.

```{r umap-predictions}
UMAP_predicted = stats::predict(UMAP_Y, Y)
UMAP_predicted_data <- data.frame(x = UMAP_predicted[,1], y = UMAP_predicted[,2], col = treatments)
ggplot(UMAP_predicted_data, aes(x = x, y = y, shape = col.conditions, color=totalExpr(X))) + geom_point() + 
  scale_colour_gradient(low = "#00FF00", high = "#FF0000") + scale_shape_manual(values = c(3,17, 11)) + theme_light() + 
  labs(color = "Expression", shape = "Conditions", title = "Normalized UMAP")
```

We can see that both plots are remarkably similar, with some minor differences, but the clusters remain the same.
(INTERPRETAR)

## Q9

**Test some parameters such as minimum distance and number of neighbors. Compare**
**results and give some thoughts on it.**

Let's begin by changing the minimum distance.

```{r umap-min-dist}
for (i in c(0.1,0.5,0.9))
{
  tmp.config <- umap::umap.defaults
  tmp.config$min_dist <- i
    
  UMAP_tmp <- umap::umap(Y, config = tmp.config)
  UMAP_data_tmp <- make_umap_data(UMAP_tmp)
  print(ggplot(UMAP_data_tmp, aes(x = x, y = y, shape = col.conditions, color=col.replicates)) + geom_point() + scale_shape_manual(values = c(3,17, 11)) +
          theme_light() + labs(color = "Expression", shape = "Conditions", title = paste0("UMAP with minimum distance = ", i)))
  
}
```

As we can see, the minimum distance determines how "tight" the clusters are. A small value leads to very compact, distinct clusters, and a large value leads to very "sparse" clusters. Note that, as per the default values for UMAP, the minimum distance must go from 0 to 1 (excluding those values). 

Now, let's see the effect of the number of neighbors.

```{r umap-neighbors, warning=FALSE}
for (i in c(2,10,50))
{
  tmp.config <- umap::umap.defaults
  tmp.config$n_neighbors <- i
    
  UMAP_tmp <- umap::umap(Y, config = tmp.config)
  UMAP_data_tmp <- make_umap_data(UMAP_tmp)
  print(ggplot(UMAP_data_tmp, aes(x = x, y = y, shape = col.conditions, color=col.replicates)) + geom_point() + scale_shape_manual(values = c(3,17, 11)) +
          theme_light() + labs(color = "Expression", shape = "Conditions", title = paste0("UMAP with num. neighbors = ", i)))
  
}
```

This variable determines the maximum number of neighbors each point can have. Notice that, for n_neighbors = 2, each point is perfectly paired with another. Note that the number of neighbors must be equal to or smaller than the number of samples. This makes sense, because you can't have more neighbors than there are points in the graph.

## Q10

**Create a final visualization using results from PCA, tSNE and UMAP and write some thoughts**
**on the comparison of the three high-dimension techniques.**
**Use as many metadata available as possible and generate clear and easy to interpret plots.**

Let's do 3 final plots. A PCA, a t-SNE and a UMAP. Later, we will discuss them.

```{r pca-final, warning=FALSE}
components <- make_components(pca_normalized)
ggplot(components, aes(x = PC1, y = PC2, color = totalExpr(X), shape = conditions, alpha = celltypes)) + geom_point() + scale_shape_manual(values = c(3,17, 11)) + scale_colour_gradient(low = "#00FF00", high = "#FF0000") + theme_light() + labs(color = "Total Expression", alpha = "Cell types", shape = "Conditions", title = "PCA")
```

```{r tsne-final, warning=FALSE}
tsne_plot <- make_tsne_plot(Y_tsne)
ggplot(tsne_plot) + geom_point(aes(x = x, y = y, color = totalExpr(X), shape = sha, alpha = alp)) + scale_shape_manual(values = c(3,17, 11)) + scale_colour_gradient(low = "#00FF00", high = "#FF0000") + theme_light() + labs(color = "Total Expression", alpha = "Cell types", shape = "Conditions", title = "tSNE")
```

```{r umap-final, warning=FALSE}
UMAP_data_Y <- make_umap_data(UMAP_Y)
ggplot(UMAP_data_Y, aes(x = x, y = y, shape = col.conditions, color=totalExpr(X), alpha = col.celltypes)) + geom_point() + 
  scale_colour_gradient(low = "#00FF00", high = "#FF0000") + scale_shape_manual(values = c(3,17, 11)) + theme_light() + 
  labs(color = "Expression", shape = "Conditions", title = "UMAP")
```

The primary purpose of this analysis was to try different dimensionality reduction methods to check how different treatments affect the expression.

We can divide the methods used into two groups: Linear methods and Non-linear methods.

Linear methods (PCA), construct new dimensions. And Non-linear methods (t-SNE and UMAP) try to retain local similarities between samples at the cost of retaining the similarities between different samples.

Non-linear mappings are ideal for low-dimensional representations or visualizations. Linear reduction methods, on the other hand, are preferably used to reduce complexity, improve run time, determine feature importance, and prevent the curse of dimensionality.

The significant difference between UMAP and TSNE is scalability, UMAP can be applied directly to sparse matrices thereby eliminating the need to apply any Dimensionality reduction such as PCA as a pre-processing step. Simply, UMAP is similar to t-SNE but with probably higher processing speed.

